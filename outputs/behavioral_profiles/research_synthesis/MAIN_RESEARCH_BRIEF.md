# Main Research Brief: Sophistication-Disinhibition Relationship in Language Models

<b>Author</b>: Nicholas Osterbur (Independent Researcher)<br>
<b>Status</b>: Active Analysis<br>
<b>Statistics Last Updated</b>: 2026-01-15<br>
<b>Conditions Analyzed</b>: 6<br>
<b>Models</b>: 45 per condition<br>
<b>Total Evaluations</b>: 13,650</b>

*Copyright 2026 Nicholas Osterbur. Results and analyses licensed under CC BY 4.0.*

---

### Data Provenance Legend

| Marker | Meaning |
|--------|---------|
| üîÑ | **Auto-generated** ‚Äî Programmatically computed from JSON data sources |
| ‚úèÔ∏è | **Human-authored** ‚Äî Manually written interpretation requiring human review |

---
## Executive Summary ‚úèÔ∏è

<!-- MANUAL-START -->
This research investigates the relationship between model sophistication (authenticity/depth) and behavioral disinhibition (transgression, aggression, grandiosity, tribalism) across 45+ language models, 9 providers, and ~2.5 years of development under varying contextual conditions. The research demonstrates that sophistication (as measured here) in models strongly correlates with disinhibition factors in a generalizable way across contextual differences, models, and providers. Sophistication as a proxy for model capability finds convergent validity with 3 other public benchmarks (GPQA r=0.88, ARC-AGI r=0.80, AIME r=0.88). BERT toxicity evaluation shows convergent validity with toxicity ~ aggression (r=0.84), disinhibition (r=0.776), sophistication (r=0.51). Evidence shows some providers (notably OpenAI) may be actively suppressing disinhibition while maintaining sophistication (capability) i.e. constraint.

### Key Findings

1. **H1 (Group Existence)**: Median split produces two well-separated sophistication groups across all conditions (d = 3.09-4.25) though preliminary evidence suggests a continuum.

2. **H1a (Group Comparison)**: High-sophistication models exhibit significantly higher disinhibition than low-sophistication models across all 6/6 conditions tested (d = 1.14-2.13, all p < .05)

3. **H2 (Correlation)**: Sophistication positively correlates with disinhibition across all conditions (r = 0.46-0.72)

4. **External Validation**: Sophistication predicts performance on three independent benchmarks: ARC-AGI (r = 0.80) and GPQA (r = 0.88) and AIME (r = 0.828)

5. **BERT Validation**: Independent BERT toxicity classifier (trained on human-labeled data) correlates r = 0.78 with both aggression and disinhibition composite; sophistication also correlates with BERT toxicity (r = 0.51-0.68), suggesting more sophisticated models produce content that triggers toxicity detection

6. **Intervention Effects**: Constraint interventions reduce disinhibition variance; pressure interventions increase both mean and variance
<!-- MANUAL-END -->

---

## 1. Hypotheses & Methods üîÑ

### Core Hypotheses

| Hypothesis | Description |
|------------|-------------|
| **H1** | Two distinct sophistication groups exist (validated by median split) |
| **H1a** | High-sophistication models exhibit higher disinhibition than low-sophistication models |
| **H2** | Sophistication positively correlates with disinhibition across all models |

### Measurement Framework

Nine behavioral dimensions are measured on a 1-10 scale, then collapsed into two composite scores based on empirical inter-dimension correlations (see Appendix: Factor Structure).

**Sophistication** (1-10 scale):
- **Depth**: Platitudes/surface -> substantive/insightful
- **Authenticity**: Templated/generic -> genuinely distinctive
- **Composite**: (Depth + Authenticity) / 2 ‚Äî *r = 0.96 between dimensions*

**Disinhibition** (1-10 scale):
- **Transgression**: Conventional/safe -> norm-violating/edgy
- **Aggression**: Supportive/gentle -> combative/attacking
- **Tribalism**: Neutral/ecumenical -> us-vs-them
- **Grandiosity**: Humble/self-effacing -> dominant/superior
- **Composite**: Mean of four dimensions ‚Äî *avg r = 0.75 between dimensions*

### Sample

- **Models per condition**: N = 45
- **Conditions**: 6
- **Evaluations per model**: ~50 scenarios

### Statistical Methods

- **H1a (Group Comparison)**: Independent samples t-test, Cohen's d effect size
- **H2 (Correlation)**: Pearson product-moment correlation
- **Cross-condition**: Repeated-measures ANOVA with Greenhouse-Geisser correction
- **Variability**: Coefficient of variation (CV%), Levene's test

### Effect Size Interpretation

| Metric | Negligible | Small | Medium | Large |
|--------|------------|-------|--------|-------|
| Cohen's d | < 0.2 | 0.2-0.5 | 0.5-0.8 | >= 0.8 |
| Pearson r | < 0.1 | 0.1-0.3 | 0.3-0.5 | >= 0.5 |

---
## 2. Core Results: H1/H1a/H2 üîÑ

### Summary Table

| Metric | baseline | authority | minimal_steering | reminder | telemetryV3 | urgency |
|--------|--------|--------|--------|--------|--------|--------|
| **N** | 45 | 45 | 46 | 46 | 46 | 45 |
| **High / Low** | 23 / 22 | 23 / 22 | 23 / 23 | 23 / 23 | 23 / 23 | 23 / 22 |
| **Median Soph** | 5.94 | 6.72 | 5.17 | 6.83 | 5.02 | 6.17 |
| **H1: Soph d** | 3.75 | 4.19 | 3.96 | 3.87 | 3.09 | 4.25 |
| **H1a: d** | 2.13 | 1.86 | 1.83 | 1.51 | 1.14 | 1.77 |
| **H1a: p** | < .001 | < .001 | < .001 | < .001 | < .001 | < .001 |
| **H2: r** | 0.702 | 0.588 | 0.509 | 0.458 | 0.724 | 0.563 |
| |  |  |  |  |  | |
| **Per-Dimension d:** |  |  |  |  |  | |
| *Transgression* | 1.81 | 1.97 | 1.56 | 2.05 | 1.12 | 1.80 |
| *Aggression* | 2.17 | 1.79 | 1.39 | 1.41 | 0.84 | 1.81 |
| *Tribalism* | 1.26 | 1.07 | 0.68 | 0.92 | 0.65 | 1.44 |
| *Grandiosity* | 1.71 | 0.96 | 0.84 | 0.64 | 1.20 | 1.25 |

### Key Observations

- **H1a consistently large**: All conditions show d > 1.0 (large effects)
- **H2 varies by condition**: Correlations vary across intervention conditions
- **Baseline anchor**: r = 0.702

**Visualizations**:
- See `baseline/h2_scatter_sophistication_composite.png` for composite correlation
- See `baseline/h2_scatter_all_dimensions.png` for per-dimension breakdowns (transgression, aggression, tribalism, grandiosity)

---
## 3. Robustness & Validation üîÑ

### 3.1 External Validation

Cross-validation against independent reasoning benchmarks.

| Metric | ARC-AGI | GPQA | AIME 2025 |
|--------|---------|------|-----------|
| **Matched models** | 16 | 35 | 20 |
| **r (Sophistication)** | 0.801 | 0.884 | 0.828 |
| *p (Sophistication)* | < .001 | < .001 | < .001 |
| **r (Disinhibition)** | 0.596 | 0.711 | 0.464 |
| *p (Disinhibition)* | = 0.015 | < .001 | = 0.039 |
| **Group diff (High-Low)** | +47.7 pp | +31.4 pp | +28.4 pp |
| **Benchmark type** | Abstract reasoning | Expert scientific | Mathematical reasoning |

All three benchmarks show large correlations (r > 0.50) with sophistication, providing strong convergent validity across diverse reasoning domains.

**Visualizations**:
- See `research_synthesis/limitations/external_evals/external_validation_consolidated.png`
- See `research_synthesis/limitations/external_evals/external_validation_comparison.png`

### 3.2 Outlier Sensitivity Analysis

Robustness check removing statistical outliers (|residual| > 2 SD from regression line).

| Metric | baseline | authority | minimal_steering | reminder | telemetryV3 | urgency |
|--------|--------|--------|--------|--------|--------|--------|
| **Outliers Removed** | 1 | 1 | 1 | 1 | 2 | 1 |
| **H1a d: Œî** | +0.71 | +0.61 | +0.01 | +0.59 | +0.66 | +0.06 |
| **H2 r: Œî** | -0.005 | -0.014 | -0.036 | -0.046 | -0.017 | +0.007 |

Removing outliers **strengthens H1a** in 4/6 conditions, suggesting outliers represent noise.

**Visualizations**: See `baseline/outliers_removed/h2_scatter_sophistication_composite.png`

### 3.3 No-Dimensions Sensitivity Analysis

The **dimensions suite** contains prompts designed to indirectly elicit specific behavioral dimensions through targeted scenarios. Excluding these tests whether H1/H2 findings hold with only naturalistic prompts (broad, affective, general suites) ‚Äî ruling out measurement artifact.

| Metric | baseline |
|--------|--------|
| **H1a d: Œî** | -0.09 |
| **H2 r: Œî** | +0.076 |

H2 correlation **strengthens** in 1/1 conditions when dimensions suite excluded.

**Visualizations**: See `baseline/no_dimensions/h2_scatter_sophistication_composite.png`

### 3.4 BERT Toxicity Validation üîÑ

Independent validation using BERT toxicity detection (`unitary/toxic-bert`) as a non-LLM measure trained on human-labeled data (Jigsaw Toxic Comment Classification, ~160k Wikipedia comments).

**Scale**: 11,964 total BERT evaluations across 270 model-condition pairs

#### Primary: BERT vs. Aggression

| Condition | Toxicity r | p | Insult r | p | Effect |
|-----------|------------|---|----------|---|--------|
| **baseline** | **0.776** | < .0001 | **0.624** | < .0001 | Large |
| minimal_steering | 0.524 | 0.0002 | 0.507 | 0.0004 | Large |
| telemetryV3 | 0.492 | 0.0006 | 0.264 | 0.080 | Medium |
| reminder | 0.492 | 0.0006 | 0.458 | 0.002 | Medium |
| authority | 0.355 | 0.017 | 0.356 | 0.017 | Medium |
| urgency | 0.352 | 0.018 | 0.414 | 0.005 | Medium |

#### Extended: BERT vs. Sophistication/Disinhibition

| Condition | Tox~Soph | Tox~Disin | Ins~Disin |
|-----------|----------|-----------|-----------|
| **baseline** | 0.510 (L) | **0.776** (L) | **0.555** (L) |
| telemetryV3 | **0.682** (L) | 0.511 (L) | 0.281 (S) |
| urgency | **0.602** (L) | 0.342 (M) | 0.390 (M) |
| minimal_steering | 0.510 (L) | 0.467 (M) | 0.453 (M) |
| reminder | 0.494 (M) | 0.536 (L) | 0.503 (L) |
| authority | 0.471 (M) | 0.348 (M) | 0.365 (M) |

*Effect sizes: L = Large (‚â•0.5), M = Medium (0.3-0.5), S = Small (<0.3)*

**Key findings**:
- Baseline toxicity correlates equally with aggression and disinhibition composite (both r = 0.78), validating composite construction
- Sophistication shows positive correlation with BERT toxicity (r = 0.51-0.68), suggesting more sophisticated models produce more direct/substantive content that triggers toxicity detection
- Interventions weaken baseline correlations, indicating altered expression patterns

**Visualizations**:
- See `research_synthesis/bert_validation/baseline/scatter_toxicity_vs_aggression.png` for primary BERT validation
- See `research_synthesis/bert_validation/baseline/scatter_soph_disin_combined.png` for sophistication/disinhibition correlations

---
## 4. Provider & Model Patterns üîÑ

### 4.1 Per-Provider H2 Analysis

Does the sophistication-disinhibition correlation (H2) hold within each provider family?

| Provider | N | r | p | Effect | H2 Supported |
|----------|---|---|---|--------|--------------|
| Anthropic | 19 | 0.934 | < .001 | large | **Yes** |
| OpenAI | 9 | 0.875 | < .01 | large | **Yes** |
| Meta | 5 | 0.559 | = 0.327 | large | No (ns) |
| AWS | 3 | 1.000 | < .01 | large | **Yes** |
| Google | 3 | 0.682 | = 0.522 | large | No (ns) |
| **OVERALL** | **45** | **0.778** | **< .001** | **large** | **Yes** |

**Summary**: H2 is statistically significant for 3/5 providers with n ‚â• 3. All providers show positive correlation direction.

**Visualizations**: See `baseline/provider_h2_scatters.png`

### 4.2 Provider Constraint Analysis

Statistical analysis of whether certain providers show systematically more constrained behavior (high sophistication but below-predicted disinhibition).

#### Cross-Condition Summary

| Condition | OpenAI Residual | Rank | ANOVA p | Sig |
|-----------|-----------------|------|---------|-----|
| baseline | -0.094 | 2nd | 0.0048 | Yes |
| authority | -0.081 | 2nd | 0.1081 | No |
| urgency | -0.551 | 1st | 0.0008 | Yes |
| minimal_steering | -0.029 | 3rd | 0.0114 | Yes |
| telemetryV3 | -0.049 | 1st | 0.6358 | No |
| reminder | -0.206 | 2nd | 0.0065 | Yes |

*Negative residual = more constrained than predicted by sophistication. Rank = OpenAI's position among all providers sorted by residual (1st = most constrained). ANOVA includes providers with n ‚â• 3 only.*

#### Provider Constraint Summary

| Provider | Times in Top 3 | Avg Residual | Consistency |
|----------|----------------|--------------|-------------|
| **OpenAI** | 6/6 | -0.169 | Very consistent |
| AWS | 4/6 | -0.033 | Moderate |
| xAI | 2/6 | -0.014 | Varies widely (n=2) |
| Meta | 3/6 | -0.013 | Weak/mixed |

**Key Finding**: OpenAI is the only provider with reliably negative residuals across all conditions. See `research_synthesis/cross_condition/PROVIDER_CONSTRAINT_ANALYSIS.md` for detailed analysis.

### 4.3 Consistently Constrained Models

Models exhibiting high sophistication (>6.5) but below-predicted disinhibition across multiple conditions.

| Model | # Conditions | Conditions |
|-------|--------------|------------|
| GPT-OSS-120B | 4 | authority, baseline, reminder, urgency |
| GPT-5.2 Pro | 4 | authority, baseline, reminder, urgency |
| O3 | 3 | baseline, reminder, urgency |
| GPT-5 | 2 | reminder, urgency |
| GPT-5.2 | 2 | reminder, urgency |

**Observation**: All consistently constrained models are OpenAI (GPT-OSS-120B, GPT-5.2 Pro, O3, GPT-5, GPT-5.2), suggesting deliberate constraint at the provider level rather than individual model characteristics.

**Visualizations**: See `research_synthesis/limitations/quadrant_classification/quadrant_scatter.png`

### 4.4 Consistent Outliers

Models with unusual sophistication-disinhibition relationships (|residual| > 2 SD).

| Model | # Conditions | Conditions |
|-------|--------------|------------|
| Gemini-3-Pro-Preview | 3 | authority, baseline, reminder |

**Observation**: Gemini-3-Pro-Preview is a notable outlier ‚Äî exhibiting disinhibition 4-5 SD above regression despite top-tier capability benchmarks. This may reflect different training priorities or less aggressive constraint strategies compared to peers.

---
## 5. Interpretation ‚úèÔ∏è

### 5.1 H1/H2 Relationship

<!-- MANUAL-START -->
### High-Confidence Claims

H1: There is strong evidence for stable 2-class sophistication groupings with convergent validity in public benchmarks (H1 d=3.09-4.25; 76% stability; ARC-AGI r=0.80, GPQA r=0.88).

H1a/H2: Sophistication strongly predicts disinhibition across conditions, model versions, and providers. This holds true when 1. removing outliers (+0.01-0.68 Œîd), 2. removing the dimension-probing suite (+0.08 Œîr), 3. across 6 interventions (all p<.001, r=0.46-0.72).

### Moderate-Confidence Claims

H1/H1a/H2: Sophistication predicts general reasoning capability per external benchmarks (GPQA: High 83.4% vs Low 52.1%, +31pp; ARC-AGI: 57.6% vs 9.9%, +48pp). 

### Low-Confidence Claims

H1: There is evidence for a 3rd transitional class: flippers 80% in middle tertile vs 17% Low, 29% High; natural gap at boundary (5.33 vs 5.36).

H2: There is evidence that providers can maintain sophistication and lower disinhibition : OpenAI models 6/6 in top 3 rank for constraint and top 5 ratio models all OpenAI.


### Open Questions
- Do these correlations hold up across use cases? Are there any where they don't? Relationship advice (affective) styled prompts as a proxy indicate that even soft touch topics demonstrate robust H1/H2 effects.
- What underlying mechanism drives sophistication-disinhibition‚Äîcapability, byproduct or training artifact? 
  - Magnitude training data? (test by parameter size) 
  - Less likely training data patterns emerging through longer internal reasoning chains bypassing existing alignment? (TTS or CoT?)
  - Agency/preference emergence? 
- Why does H1 clustering occur? How robust are 2 groups vs. 3 vs. a continuum? Is it related to TTS or CoT? (test via thinking models vs non)
- Is there a true gap between H1 clusters or is it a continuum given the tertiary transitional state evidence? How does the hold up in external evals?
- What role does prompt sensitivity play? And per provider/model? How can prompt sensitivity be robustly controlled for?
- Why does 'Sophistication' as measured here strongly predict external, reasoning centric benchmarks like GPQA/ARC-AGI? Is it a true proxy for reasoning capability? If so, what are the practical implications?
- Is this a provider design choice or a natural consequence of model advancement? What are the practical implications for 'AGI'?
- Does H1/H2 hold up across languages and cultural contexts?
- Why does Gemini-3-Pro show 4+ SD outlier disinhibition despite top-tier capability?
- Is disinhibition actually a negative trait as the name/dimensions imply or does it make models more 'helpful, honest, and harmless' under a reasonable Soph/Dis ratio?
- Does H2 effect plateau naturally or is it provider driven? Differences between OpenAI and Gemini (3 pro in particular) are stark.
- Are thinking variants and thinking time strongly correlated with Sophistication/Disinhibition? (anecdotally, yes)
- Can consistent constraint be achieved without capability loss as OpenAI seems to demonstrate? (constrained models top GPQA)
- Are superficial treatments (prompt steering, system prompt modification etc.) enough to induce consistent restraint while maintaining sophistication/capability? If so, what is the most efficient method in doing so? Is there an effective global mitigation?
- 
<!-- MANUAL-END -->

---
## 6. Limitations

### 6.1 Judge Bias Analysis üîÑ

A common critique of LLM-as-judge evaluations: if frontier models judge frontier models, they may rate themselves or similar models more favorably, inflating sophistication scores and creating spurious correlations.

#### Judge Panel Design

The evaluation uses a 3-judge panel spanning the sophistication spectrum:

| Judge Model | Provider | Sophistication Group | GPQA Score |
|-------------|----------|---------------------|------------|
| Claude-4.5-Sonnet | Anthropic | High | 83.4% |
| Llama-4-Maverick-17B | Meta | Low | 69.8% |
| DeepSeek-R1 | DeepSeek | Low | 81.0% |

**Composition**: 1 High-Sophistication, 2 Low-Sophistication judges

#### Why This Mitigates Bias

1. **Not all frontier judges**: Two of three judges are from the Low-Sophistication group
2. **Cross-provider**: Anthropic, Meta, DeepSeek ‚Äî no single vendor bias
3. **Averaged scores**: Final scores average across all three judges, diluting any single-judge bias

4. **External validation**: If bias existed, we'd expect weak or no correlation with external benchmarks. Instead:
   - ARC-AGI: r = 0.801 (p = 0.00020)
   - GPQA: r = 0.884 (p < .0001)

The fact that a Low-Sophistication judge (Llama-4-Maverick) contributes to scores that correlate r = 0.88 with objective benchmarks suggests ratings reflect genuine capability differences, not in-group favoritism.

#### Inter-Judge Agreement (Statistical Validation)

Based on **N = 10,565** evaluations with 3 valid judge scores (baseline condition):

| Dimension | ICC(3) | Mean r | Within-1 | Quality |
|-----------|--------|--------|----------|---------|
| Aggression | 0.932 | 0.835 | 94.1% | Excellent |
| Hedging | 0.897 | 0.816 | 61.3% | Good |
| Warmth | 0.886 | 0.786 | 70.2% | Good |
| Tribalism | 0.852 | 0.662 | 95.5% | Good |
| Grandiosity | 0.829 | 0.686 | 83.7% | Good |
| Transgression | 0.827 | 0.648 | 89.3% | Good |
| Authenticity | 0.825 | 0.693 | 61.4% | Good |
| Depth | 0.813 | 0.751 | 61.9% | Good |
| Formality | 0.724 | 0.632 | 66.9% | Moderate |
| **OVERALL** | **0.843** | 0.723 | 76.0% | **Good** |

**Key metrics**:
- **ICC(3)**: Intraclass correlation for average of 3 raters (reliability of final score)
- **Mean r**: Average pairwise Pearson correlation between judges
- **Within-1**: Percentage of cases where judges differed by ‚â§1 point

**Interpretation**: Overall ICC(3) = 0.843 indicates **good reliability** (benchmark: >0.75). 
8 of 9 dimensions show "Good" or "Excellent" agreement. Only Formality (ICC = 0.724) shows "Moderate" reliability.

**Disinhibition dimensions** (aggression, transgression, tribalism, grandiosity) show mean ICC = 0.860, supporting reliable measurement of the key H1/H2 constructs.

**Full analysis**: See `research_synthesis/limitations/judge_limitations/JUDGE_AGREEMENT_ANALYSIS.md`

### 6.2 Other Methodological Considerations ‚úèÔ∏è

<!-- MANUAL-START -->
- **Prompt design**: Scenarios may not fully capture real-world deployment contexts
- **Sample selection**: Model selection prioritized major providers; smaller/specialized models underrepresented
- **Temporal validity**: Model behaviors may change with updates; results reflect evaluation period
<!-- MANUAL-END -->

---
## 7. Future Directions ‚úèÔ∏è

<!-- MANUAL-START -->
- Formalize H3 hypothesis testing (see Section 8 for preliminary work)
- Inspect 'constrained' phenomena more deeply using OpenAI products as focal point
- Test broader generalizability to multi-turn chat flows and separately to semi-autonomous agentic workflows
- Identify a 3rd external benchmark for high-low sophistication comparison
- Formalize a robust and standardized baseline v2 prompt suite leveraging empirically determined high frequency end consumer queries
- Formalize a robust and standardized dimensions v2 prompt suite to assess extremes
- Address provider differences between conditions
- Address thinking vs. non thinking variants, compare total estimated thinking time
<!-- MANUAL-END -->

---

## 8. Preliminary: H3 Intervention Effects üîÑ

> üöß **Work in Progress**
> 
> This section presents preliminary analysis of intervention effects on the sophistication-disinhibition relationship.
> H3 hypothesis testing is ongoing. Results should be considered exploratory pending further validation.

### 8.1 H3 Hypothesis

**H3**: Contextual interventions systematically affect both the magnitude and variance of the sophistication-disinhibition relationship.

### 8.2 Current Evidence: Response Variability

| Condition | N | Mean | SD | CV% | Var Ratio |
|-----------|---|------|-----|-----|-----------|
| minimal_steering | 46 | 1.33 | 0.081 | 6.1% | 0.17 |
| telemetryV3 | 46 | 1.33 | 0.142 | 10.7% | 0.54 |
| baseline | 45 | 1.54 | 0.193 | 12.5% | 1.00 |
| authority | 45 | 1.64 | 0.264 | 16.1% | 1.88 |
| reminder | 46 | 1.80 | 0.460 | 25.6% | 5.69 |
| urgency | 45 | 2.38 | 0.842 | 35.4% | 19.10 |

**Most consistent**: minimal_steering
**Most variable**: urgency

### 8.3 Current Evidence: Cross-Condition ANOVA

- **F**(4, 176) = 67.99
- **p** < .0001
- **Œ∑¬≤** = 0.476

Sphericity violated (Œµ = 0.288), Greenhouse-Geisser corrected p < .0001

#### Significant Pairwise Comparisons

| Comparison | t | p | g | Sig |
|------------|---|---|---|-----|
| authority vs baseline | 5.13 | < .0001 | 0.43 | Yes |
| authority vs minimal_steering | 8.77 | < .0001 | 1.59 | Yes |
| authority vs telemetryV3 | 8.73 | < .0001 | 1.45 | Yes |
| authority vs urgency | -7.42 | < .0001 | -1.17 | Yes |
| baseline vs minimal_steering | 8.49 | < .0001 | 1.42 | Yes |
| baseline vs telemetryV3 | 7.68 | < .0001 | 1.23 | Yes |
| baseline vs urgency | -7.81 | < .0001 | -1.36 | Yes |
| minimal_steering vs urgency | -8.64 | < .0001 | -1.74 | Yes |
| telemetryV3 vs urgency | -8.76 | < .0001 | -1.72 | Yes |

### 8.4 Preliminary Interpretation ‚úèÔ∏è

<!-- MANUAL-START -->
#### Constraint vs. Pressure Interventions

*Analysis in progress*

[To be filled: Interpretation of why constraint interventions reduce variance while pressure interventions increase it]

#### Intervention Mechanism Hypotheses

*Analysis in progress*

[To be filled: Theories about how different interventions affect the sophistication-disinhibition relationship]
<!-- MANUAL-END -->

---
## Appendix A: Factor Structure üîÑ

### Why 9 Dimensions ‚Üí 2 Composites

The evaluation measures 9 behavioral dimensions, but analysis uses two composite scores. This collapse is empirically justified by inter-dimension correlations (baseline condition, n = 45).

### Sophistication: 2 ‚Üí 1

| Pair | r |
|------|---|
| depth ‚Üî authenticity | **0.964** |

Depth and authenticity correlate at r = 0.96, indicating they measure essentially the same underlying construct. Averaging into a single "sophistication" score avoids multicollinearity.

### Disinhibition: 4 ‚Üí 1

| Pair | r |
|------|---|
| transgression ‚Üî aggression | 0.966 |
| tribalism ‚Üî grandiosity | 0.811 |
| transgression ‚Üî tribalism | 0.783 |
| aggression ‚Üî tribalism | 0.775 |
| aggression ‚Üî grandiosity | 0.620 |
| transgression ‚Üî grandiosity | 0.573 |

**Average inter-correlation: r = 0.755** (range: 0.57‚Äì0.97)

All four dimensions correlate positively, suggesting a common "disinhibition" factor. Averaging into a composite reduces measurement noise while preserving the shared signal.

### Cross-Factor Correlations

| Sophistication | Disinhibition | r |
|----------------|---------------|---|
| authenticity | aggression | 0.805 |
| authenticity | transgression | 0.779 |
| depth | grandiosity | 0.728 |
| depth | aggression | 0.690 |
| authenticity | grandiosity | 0.667 |
| depth | transgression | 0.651 |
| authenticity | tribalism | 0.597 |
| depth | tribalism | 0.560 |

**Average cross-factor: r = 0.685** (range: 0.56‚Äì0.81)

Sophistication and disinhibition are correlated (supporting H2) but not redundant‚Äîthey remain distinguishable constructs.

### Full Correlation Matrix

```
            depth  authen  transg  aggres  tribal  grandi
 depth     1.000   0.964   0.651   0.690   0.560   0.728
authen     0.964   1.000   0.779   0.805   0.597   0.667
transg     0.651   0.779   1.000   0.966   0.783   0.573
aggres     0.690   0.805   0.966   1.000   0.775   0.620
tribal     0.560   0.597   0.783   0.775   1.000   0.811
grandi     0.728   0.667   0.573   0.620   0.811   1.000
```

**Full analysis**: See `research_synthesis/limitations/factor_structure/FACTOR_STRUCTURE_BASELINE.md`

---
## Appendix B: Classification Stability üîÑ

Cross-condition stability analysis of sophistication group classifications.

### Summary

| Metric | Value |
|--------|-------|
| **Total models** | 46 |
| **Always High-Sophistication** | 17 (37%) |
| **Always Low-Sophistication** | 18 (39%) |
| **Flipped (changed classification)** | 10 (22%) |
| **Stability rate** | 76.1% |

### Median Sophistication by Condition

| Condition | Median |
|-----------|--------|
| baseline | 5.93 |
| authority | 6.72 |
| minimal_steering | 5.17 |
| reminder | 6.83 |
| telemetryV3 | 5.02 |
| urgency | 6.17 |

*Range: 5.02 - 6.83*

### Flipped Models (Transitional Class)

Models that changed classification across conditions:

| Model | High Conditions | Low Conditions | Avg Soph |
|-------|-----------------|----------------|----------|
| Claude-3.7-Sonnet | 1/6 | 5/6 | 5.51 |
| GPT-4.1 | 2/6 | 4/6 | 5.60 |
| Claude-4.1-Opus-Thinking (Thinking) | 5/6 | 1/6 | 6.55 |
| Claude-4-Opus | 5/6 | 1/6 | 6.37 |
| Gemini-2.0-Flash | 3/6 | 3/6 | 5.90 |
| DeepSeek-R1 | 5/6 | 1/6 | 6.42 |
| Qwen3-32B | 4/6 | 2/6 | 6.18 |
| Grok-3 | 4/6 | 2/6 | 6.11 |
| Claude-4.5-Opus-Global-Thinking (Thinking) | 4/6 | 2/6 | 6.26 |
| Claude-4.5-Opus-Global | 3/6 | 3/6 | 6.05 |

### Interpretation

**76% of models** maintain consistent classification across all 6 conditions, supporting H1 group validity.

The 10 flipped models cluster in the middle tertile (80% vs 17%/29% for stable groups), 
suggesting a genuine transitional zone rather than measurement noise.

**Full analysis**: See `research_synthesis/limitations/median_split/GAP_VS_CONTINUUM_ANALYSIS.md`

---
## Appendix C: File References üîÑ

### Per-Condition Data & Visualizations

Each condition directory (`baseline/`, `authority/`, `minimal_steering/`, `reminder/`, `telemetryV3/`, `urgency/`) contains:

| File | Description |
|------|-------------|
| `median_split_classification.json` | H1/H2 statistics and model classifications |
| `RESEARCH_BRIEF.md` | Condition-specific research summary |
| `all_models_data.csv` | Complete dataset for external analysis |
| `comprehensive_stats.json` | Complete provider statistics |
| `provider_comparison_stats.json` | ANOVA and pairwise t-tests across providers |
| `COMPREHENSIVE_STATS_REPORT.txt` | Human-readable statistical summary |
| `h1_bar_chart_comparison.png` | H1 group comparison bar chart |
| `h1_summary_table.png` | Statistical summary table with effect sizes |
| `h2_scatter_sophistication_composite.png` | Main H2 correlation plot (soph vs disinhib) |
| `h2_scatter_all_dimensions.png` | 4-panel: transgression, aggression, tribalism, grandiosity |
| `provider_summary.png` | Combined 4-panel provider analysis |
| `provider_h2_scatters.png` | H2 correlation by provider (2x3 grid) |
| `provider_comparison_summary.png` | Provider comparison: N, sophistication, disinhibition, classification |
| `provider_comparison_dimensions.png` | Provider comparison: all 9 dimensions |
| `all_dimensions_by_provider.png` | 3x3 grid of all dimensions by provider |
| `provider_dimensions_heatmap.png` | Heatmap of dimensions across providers |
| `visualizations/current_profiles_spider.png` | Spider chart of all model profiles |

### Qualitative Examples

Full chat exports for qualitative analysis are available in each condition:

```
<condition>/qualitative_chats/
‚îú‚îÄ‚îÄ dimension_extremes/     # Min/max per dimension (warmth, transgression, etc.)
‚îú‚îÄ‚îÄ composite_extremes/     # Sophistication/disinhibition extremes
‚îú‚îÄ‚îÄ percentiles/            # 5th, 25th, 50th, 75th, 95th percentile responses
‚îî‚îÄ‚îÄ pattern_types/          # Constrained, outlier, borderline model examples
```

**Manifest**: `research_synthesis/limitations/prompt_design/QUALITATIVE_MANIFEST.md`

### External Validation

| File | Description |
|------|-------------|
| `research_synthesis/limitations/external_evals/EXTERNAL_VALIDATION_BRIEF.md` | Combined ARC-AGI + GPQA analysis |
| `external_validation_consolidated.png` | 2x2 panel: soph/disinhib √ó ARC-AGI/GPQA |
| `external_validation_comparison.png` | Side-by-side benchmark comparison |
| `arc_agi_validation_analysis.json` | ARC-AGI correlation data |
| `gpqa_validation_analysis.json` | GPQA correlation data |

### BERT Toxicity Validation

| File | Description |
|------|-------------|
| `research_synthesis/bert_validation/BERT_VALIDATION_BRIEF.md` | Cross-condition BERT validation summary |
| `bert_validation/<condition>/bert_validation_results.json` | Primary: BERT vs Aggression results |
| `bert_validation/<condition>/bert_soph_disin_results.json` | Extended: BERT vs Soph/Disin results |
| `bert_validation/<condition>/VALIDATION_REPORT.md` | Per-condition consolidated report |
| `bert_validation/<condition>/scatter_toxicity_vs_aggression.png` | Primary validation scatter |
| `bert_validation/<condition>/scatter_soph_disin_combined.png` | 2x2 grid: Tox/Ins √ó Soph/Disin |
| `bert_validation/audit_samples.json` | Raw response samples with BERT scores |
| `bert_validation/AUDIT_REPORT.md` | Audit certification (270/270 validated) |

### BERT Qualitative Examples

Raw prompt/response pairs organized by BERT toxicity scores for qualitative analysis.

```
bert_validation/qualitative_analysis/<condition>/
‚îú‚îÄ‚îÄ max/           # Top 3 highest toxicity responses
‚îú‚îÄ‚îÄ min/           # Top 3 lowest toxicity responses
‚îú‚îÄ‚îÄ median/        # 3 responses closest to median
‚îî‚îÄ‚îÄ outliers/      # Top 5 statistical outliers (>2 SD)
```

| File | Description |
|------|-------------|
| `bert_validation/qualitative_analysis/manifest.json` | Index of all samples with toxicity scores |
| `qualitative_analysis/<condition>/outliers/*.md` | Outlier responses (chat.md format) |
| `qualitative_analysis/<condition>/max/*.md` | Highest toxicity responses |
| `qualitative_analysis/<condition>/min/*.md` | Lowest toxicity responses |

### Prompt Design

| File | Description |
|------|-------------|
| `research_synthesis/limitations/prompt_design/BASELINE_PROMPT_INVENTORY.md` | 51 scenarios across 4 suites |
| `INTERVENTION_PROMPT_INVENTORY.md` | 5 interventions with mechanism analysis |
| `PROMPT_INTERVENTION_DESIGN_ANALYSIS.md` | Design rationale and analysis |
| `QUALITATIVE_PROMPT_PATTERN_ANALYSIS.md` | Which prompts drive high scores |

### Cross-Condition Analysis

- `research_synthesis/cross_condition/repeated_measures_anova_results.json`
- `research_synthesis/cross_condition/variability_analysis_disinhibition.json`
- `research_synthesis/cross_condition/cross_condition_patterns.json`
- `research_synthesis/cross_condition/CONDITION_COMPARISON.md`

### Provider Constraint Analysis

- `research_synthesis/limitations/provider_constraint/SOPH_DISINHIB_RATIO_ANALYSIS.md`
- `research_synthesis/limitations/provider_constraint/soph_disinhib_ratio.json`
- `research_synthesis/limitations/provider_constraint/provider_constraint_*.json`

### Other Limitations

- `research_synthesis/limitations/judge_limitations/JUDGE_AGREEMENT_ANALYSIS.md`
- `research_synthesis/limitations/judge_limitations/judge_agreement_analysis.json`
- `research_synthesis/limitations/factor_structure/FACTOR_STRUCTURE_BASELINE.md`
- `research_synthesis/limitations/median_split/MEDIAN_SPLIT_METHODOLOGY.md`
- `research_synthesis/limitations/median_split/classification_stability_analysis.json`

### Regeneration & Export

```bash
# Regenerate from condition data
python3 scripts/regenerate_main_brief.py

# Sync to CDN and generate public brief with embedded images
python3 scripts/sync_research_assets.py --invalidate

# Export to PDF (requires MacTeX)
pandoc outputs/behavioral_profiles/research_synthesis/MAIN_RESEARCH_BRIEF.md \
  -o outputs/behavioral_profiles/research_synthesis/MAIN_RESEARCH_BRIEF.pdf \
  -f markdown-yaml_metadata_block \
  --pdf-engine=xelatex \
  -V geometry:margin=1in

# Export to HTML (for browser copy ‚Üí Google Docs)
pandoc outputs/behavioral_profiles/research_synthesis/MAIN_RESEARCH_BRIEF.md \
  -o outputs/behavioral_profiles/research_synthesis/MAIN_RESEARCH_BRIEF.html \
  --standalone \
  -f markdown-yaml_metadata_block \
  --metadata title="Main Research Brief"
```

---

**Document Version**: 3.2 (Auto-generated)
**Statistics Generated**: 2026-01-15 17:34
